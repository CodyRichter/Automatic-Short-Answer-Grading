{"cells":[{"cell_type":"markdown","metadata":{"id":"0UnfUwvwN5W6"},"source":["# Project Dataset Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tL-P3q4MaeG"},"outputs":[],"source":["import zipfile\n","import os\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import glob\n","import torch\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1027,"status":"ok","timestamp":1651862939835,"user":{"displayName":"Peilan Wang","userId":"03253077972159635505"},"user_tz":240},"id":"dQKdp7p4_ZUg","outputId":"fb8ed305-d4b7-4fe4-d22c-cc952f69e0a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'score-freetext-answer'...\n","remote: Enumerating objects: 511, done.\u001b[K\n","remote: Total 511 (delta 0), reused 0 (delta 0), pack-reused 511\u001b[K\n","Receiving objects: 100% (511/511), 478.34 KiB | 3.65 MiB/s, done.\n","Resolving deltas: 100% (263/263), done.\n"]}],"source":["# Clone the dataset repository from github\n","!git clone https://github.com/leocomelli/score-freetext-answer.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wg4HJSXW_ZXZ"},"outputs":[],"source":["training_data_directory = '/content/score-freetext-answer/src/main/resources/corpus/semeval2013-task7/training/2way/sciEntsBank'\n","test_data_unseen_ans_directory = '/content/score-freetext-answer/src/main/resources/corpus/semeval2013-task7/test/2way/sciEntsBank/test-unseen-answers'\n","test_data_unseen_que_directory = '/content/score-freetext-answer/src/main/resources/corpus/semeval2013-task7/test/2way/sciEntsBank/test-unseen-questions'\n","test_data_unseen_dom_directory = '/content/score-freetext-answer/src/main/resources/corpus/semeval2013-task7/test/2way/sciEntsBank/test-unseen-domains'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFzAxIIbFhVm"},"outputs":[],"source":["def parse_xml_file(xml_file_path):\n","\n","  question = \"\"\n","  ref = \"\"\n","  results = []\n","\n","  for elem in ET.parse(xml_file_path).getroot():\n","    if elem.tag == 'questionText':\n","      question = elem.text\n","    for subelem in elem:\n","      if subelem.tag == 'referenceAnswer':\n","        ref = subelem.text\n","      else:\n","        results.append({\n","            'question': question,\n","            'ref': ref,\n","            'response': subelem.text,\n","            'score': subelem.attrib['accuracy'],\n","            'aug': False\n","        })\n","\n","  return results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1651872365340,"user":{"displayName":"Peilan Wang","userId":"03253077972159635505"},"user_tz":240},"id":"T3Cpw_yM_ZaB","outputId":"334c1cbe-4fd7-4faf-d03a-843691dd25ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Training Questions: 135\n","Number of Training Responses: 4969\n","\n","Number of Test Questions (Unseen Answers): 135\n","Number of Test Questions (Unseen Questions): 15\n","Number of Test Questions (Unseen Domains): 46\n","\n","Number of Test Responses (Unseen Answers): 540\n","Number of Test Responses (Unseen Questions): 733\n","Number of Test Responses (Unseen Domains): 4562\n"]}],"source":["training_data = []\n","test_data_unseen_ans = []\n","test_data_unseen_que = []\n","test_data_unseen_dom = []\n","num_training_questions = 0\n","num_test_questions_unseen_ans = 0\n","num_test_questions_unseen_que = 0\n","num_test_questions_unseen_dom = 0\n","\n","for data_file in glob.glob(training_data_directory + '/*'):\n","  training_data += parse_xml_file(data_file)\n","  num_training_questions += 1\n","\n","for data_file in glob.glob(test_data_unseen_ans_directory + '/*'):\n","  test_data_unseen_ans += parse_xml_file(data_file)\n","  num_test_questions_unseen_ans += 1\n","\n","for data_file in glob.glob(test_data_unseen_que_directory + '/*'):\n","  test_data_unseen_que += parse_xml_file(data_file)\n","  num_test_questions_unseen_que += 1\n","\n","for data_file in glob.glob(test_data_unseen_dom_directory + '/*'):\n","  test_data_unseen_dom += parse_xml_file(data_file)\n","  num_test_questions_unseen_dom += 1\n","\n","\n","id_idx = 0\n","for tmp_ds in [training_data, test_data_unseen_ans, test_data_unseen_que, test_data_unseen_dom]:\n","  for data_item in tmp_ds:\n","    data_item['id'] = id_idx\n","    id_idx += 1\n","\n","\n","print(\"Number of Training Questions:\", num_training_questions)\n","print(\"Number of Training Responses:\", len(training_data))\n","print(\"\")\n","print(\"Number of Test Questions (Unseen Answers):\", num_test_questions_unseen_ans)\n","print(\"Number of Test Questions (Unseen Questions):\", num_test_questions_unseen_que)\n","print(\"Number of Test Questions (Unseen Domains):\", num_test_questions_unseen_dom)\n","print(\"\")\n","print(\"Number of Test Responses (Unseen Answers):\", len(test_data_unseen_ans))\n","print(\"Number of Test Responses (Unseen Questions):\", len(test_data_unseen_que))\n","print(\"Number of Test Responses (Unseen Domains):\", len(test_data_unseen_dom))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136,"status":"ok","timestamp":1651872368799,"user":{"displayName":"Peilan Wang","userId":"03253077972159635505"},"user_tz":240},"id":"ZmdLeeTkhohA","outputId":"504e939c-38f1-4575-f009-a55811e79735"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'question': 'The sand and flour in the gray material from mock rocks is separated by mixing with water and allowing the mixture to settle. Explain why the sand and flour separate.', 'ref': 'The sand particles are larger and settle first. The flour particles are smaller and therefore settle more slowly.', 'response': 'One is heavier than another see it settles in how many layers there is ingredients.', 'score': 'correct', 'aug': False, 'id': 0}\n"]}],"source":["print(training_data[0])"]},{"cell_type":"markdown","metadata":{"id":"jItRfPfSzKSG"},"source":["## Dataset Augmentation\n","\n","Using Google Translate, we can augment the relatively small amount of training data via Backtranslation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEclZ4HD0DXs"},"outputs":[],"source":["!pip install -q googletrans==3.1.0a0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVHePTdtzKbg"},"outputs":[],"source":["import googletrans\n","from googletrans import Translator\n","import json"]},{"cell_type":"markdown","metadata":{"id":"MSrXWU1h8goJ"},"source":["#### Question Response Map Creation\n","\n","We create this to ensure that any generated responses do not match any of the existing responses for the question. This prevents duplication and improves the quality of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtWJYGdK79XY"},"outputs":[],"source":["question_response_map = {}\n","\n","for training_item in training_data:\n","  question = training_item['question']\n","\n","  if question not in question_response_map:\n","    question_response_map[question] = set()\n","\n","  response = training_item['response']\n","  question_response_map[question].add(response)"]},{"cell_type":"markdown","metadata":{"id":"a1EMgR7A8rbo"},"source":["#### Translation Functions\n","\n","These methods will perform the augmentation of the dataset via backtranslation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99GkLybl8xtq"},"outputs":[],"source":["translator = Translator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inPxbN120P9V"},"outputs":[],"source":["def backtranslate_example(input_item):\n","\n","  global id_idx\n","\n","  original_response = input_item['response']\n","\n","  new_dataset_entries = []  # New Validated Dataset Entries\n","\n","  trs = []  # Translation Candidates\n","  tr0 = translator.translate(original_response, src='en', dest='es').text\n","  tr1 = translator.translate(tr0, src='es', dest='fr').text\n","  tr2 = translator.translate(tr1, src='fr', dest='de').text\n","  tr3 = translator.translate(tr2, src='de', dest='cs').text\n","  tr4 = translator.translate(tr3, src='cs', dest='ru').text\n","  trs.append((translator.translate(tr0, src='es', dest='en').text, ['en', 'es', 'en']))\n","  trs.append((translator.translate(tr1, src='fr', dest='en').text, ['en', 'es', 'fr', 'en']))\n","  trs.append((translator.translate(tr2, src='de', dest='en').text, ['en', 'es', 'fr', 'de', 'en']))\n","  trs.append((translator.translate(tr3, src='cs', dest='en').text, ['en', 'es', 'fr', 'de', 'cs', 'en']))\n","  trs.append((translator.translate(tr4, src='ru', dest='en').text, ['en', 'es', 'fr', 'de', 'cs', 'ru', 'en']))\n","\n","  for new_response_tuple in trs:\n","\n","    new_response = new_response_tuple[0]\n","    new_response_translation_sequence = new_response_tuple[1]\n","\n","    # Do not add the new response if it is the same after translation\n","    if new_response == original_response:\n","      continue\n","\n","    # Do not add the new response if there is already another response that is the same\n","    if new_response in question_response_map[input_item['question']]:\n","      continue\n","\n","    # Update the question response map with the new response\n","    question_response_map[input_item['question']].add(new_response)\n","\n","    # Create a new dataset entry and return it\n","    new_dataset_entry = {\n","      'question': input_item['question'],\n","      'ref': input_item['ref'],\n","      'response': new_response,\n","      'score': input_item['score'],\n","      'aug': True,\n","      'id': id_idx,\n","      'aug_metadata': {\n","          'parent_id': input_item['id'],\n","          'translation_seq': new_response_translation_sequence\n","      }\n","    }\n","\n","    id_idx += 1\n","\n","    new_dataset_entries.append(new_dataset_entry)\n","\n","  return new_dataset_entries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peUmKlkvzVuj"},"outputs":[],"source":["def process_generation(starting_idx, limit):\n","  gen_list = []\n","  for idx, training_item in enumerate(training_data):\n","    if idx < starting_idx:\n","      continue\n","\n","    augmented_items = backtranslate_example(training_item)\n","\n","    if augmented_items:\n","      gen_list += augmented_items\n","\n","    if idx % 50 == 0:\n","      print('Processed', idx)\n","\n","    if idx == starting_idx + limit:\n","      print('Finished Processing To Index', idx)\n","      return gen_list\n","  print('Finished Processing All Data')\n","  return gen_list"]},{"cell_type":"markdown","metadata":{"id":"UWY9Z0Nr5K3-"},"source":["#### Data Augmentation Implementation\n","\n","Note that we break these into multiple cells. This is due to the rate limiting with the translate method, and it benefits from having each cell called individually with down-time in between cell executions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Qr_cT25JsE"},"outputs":[],"source":["augmented_data = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TStSX4Mz38UP","outputId":"7321df76-d518-4875-e568-302d761a6f8b","executionInfo":{"status":"ok","timestamp":1651875481725,"user_tz":240,"elapsed":3073536,"user":{"displayName":"Peilan Wang","userId":"03253077972159635505"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Indices 1500 to 1999...\n","Processed 1500\n","Processed 1550\n","Processed 1600\n","Processed 1650\n","Processed 1700\n","Processed 1750\n","Processed 1800\n","Processed 1850\n","Processed 1900\n","Processed 1950\n","Finished Processing To Index 1999\n","Finished Processing Indices 1500 to 1999.\n","[Saved] Size of Augmented Dataset 1543\n","Processing Indices 2000 to 2499...\n","Processed 2000\n","Processed 2050\n","Processed 2100\n","Processed 2150\n","Processed 2200\n","Processed 2250\n","Processed 2300\n","Processed 2350\n","Processed 2400\n","Processed 2450\n","Finished Processing To Index 2499\n","Finished Processing Indices 2000 to 2499.\n","[Saved] Size of Augmented Dataset 3113\n","Processing Indices 2500 to 2999...\n","Processed 2500\n","Processed 2550\n","Processed 2600\n","Processed 2650\n","Processed 2700\n","Processed 2750\n","Processed 2800\n","Processed 2850\n","Processed 2900\n","Processed 2950\n","Finished Processing To Index 2999\n","Finished Processing Indices 2500 to 2999.\n","[Saved] Size of Augmented Dataset 4722\n","Processing Indices 3000 to 3499...\n","Processed 3000\n","Processed 3050\n","Processed 3100\n","Processed 3150\n","Processed 3200\n","Processed 3250\n","Processed 3300\n","Processed 3350\n","Processed 3400\n","Processed 3450\n","Finished Processing To Index 3499\n","Finished Processing Indices 3000 to 3499.\n","[Saved] Size of Augmented Dataset 6401\n","Processing Indices 3500 to 3999...\n","Processed 3500\n","Processed 3550\n","Processed 3600\n","Processed 3650\n","Processed 3700\n","Processed 3750\n","Processed 3800\n","Processed 3850\n","Processed 3900\n","Processed 3950\n","Finished Processing To Index 3999\n","Finished Processing Indices 3500 to 3999.\n","[Saved] Size of Augmented Dataset 8062\n","Processing Indices 4000 to 4499...\n","Processed 4000\n","Processed 4050\n","Processed 4100\n","Processed 4150\n","Processed 4200\n","Processed 4250\n","Processed 4300\n","Processed 4350\n","Processed 4400\n","Processed 4450\n","Finished Processing To Index 4499\n","Finished Processing Indices 4000 to 4499.\n","[Saved] Size of Augmented Dataset 9689\n","Processing Indices 4500 to 4999...\n","Processed 4500\n","Processed 4550\n","Processed 4600\n","Processed 4650\n","Processed 4700\n","Processed 4750\n","Processed 4800\n","Processed 4850\n","Processed 4900\n","Processed 4950\n","Finished Processing All Data\n","Finished Processing Indices 4500 to 4999.\n","[Saved] Size of Augmented Dataset 11296\n"]}],"source":["for i in range(3, 10):\n","\n","  start_idx = 500*i\n","\n","  print(f'Processing Indices {start_idx} to {start_idx + 499}...')\n","  generated_data_samples = process_generation(start_idx, 499)\n","  print(f'Finished Processing Indices {start_idx} to {start_idx + 499}.')\n","\n","  if len(generated_data_samples) == 0:\n","    print(\"Nothing More To Process... Terminating.\")\n","    break\n","\n","  augmented_data += generated_data_samples  # Save Results\n","\n","  # Save augmented training data every 500x\n","  with open(f'/content/train_to_idx_{start_idx+499}.json', 'w') as fp:\n","      json.dump(augmented_data, fp, indent=4)\n","\n","  print('[Saved] Size of Augmented Dataset', len(augmented_data))"]},{"cell_type":"code","source":["with open(f'/content/train_augmented_complete.json', 'w') as fp:\n","    json.dump(training_data + augmented_data, fp, indent=4)"],"metadata":{"id":"ltDuQHRpbMRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udkCFBMg11vz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651875535383,"user_tz":240,"elapsed":115,"user":{"displayName":"Peilan Wang","userId":"03253077972159635505"}},"outputId":"c2276718-e5fa-4934-f36e-a454dc80c2de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Original  Dataset 4969\n","Size of Combined  Dataset 16265\n","Dataset Size Increase 227.3294425437714%\n"]}],"source":["print('Size of Original  Dataset', len(training_data))\n","\n","print('Size of Combined  Dataset', len(training_data) + len(augmented_data))\n","\n","print(f'Dataset Size Increase {100*len(augmented_data)/len(training_data)}%')"]},{"cell_type":"markdown","metadata":{"id":"C-zgIXxzELjw"},"source":["## Results\n","\n","We have increased the size of our training set by almost 80%! This is a big improve that we hope will lead to better results in model training and fine-tuning.\n","\n","Now, save all of the files as .json for easier use in the future"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IPN7ZDXBK5W"},"outputs":[],"source":["# Save augmented training data\n","with open('/content/train.json', 'w') as fp:\n","    json.dump(augmented_data + training_data, fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hI5GblNcEYmy"},"outputs":[],"source":["# Save augmented training data\n","with open('/content/test-unseen-answers.json', 'w') as fp:\n","    json.dump(test_data_unseen_ans, fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-n1zW0lEYvs"},"outputs":[],"source":["# Save augmented training data\n","with open('/content/test-unseen-questions.json', 'w') as fp:\n","    json.dump(test_data_unseen_que, fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJIAwjZyos6V"},"outputs":[],"source":["# Save augmented training data\n","with open('/content/test-unseen-domains.json', 'w') as fp:\n","    json.dump(test_data_unseen_dom, fp)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Dataset Transformation + Augmentation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}